{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Neural Network\n",
    "\n",
    "This code significantly overlaps with the code from Project 5: Image Classification, in another [GitHub folder](https://github.com/dangall/Udacity-Machine-Learning-Nanodegree/blob/master/P5_image_classification/image_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_training_data(batch_number, image_numbers=[]):\n",
    "    \"\"\"\n",
    "    Loads the training data from files. It is possible to specify an interval \n",
    "    of images to load, or by defauly load th entire batch.\n",
    "    \"\"\"\n",
    "    if image_numbers == []:\n",
    "        return np.load(\"./TensorFlow_data/training_data/training_images_batch\" + str(batch_number) + \".npy\")\n",
    "    else:\n",
    "        return np.load(\"./TensorFlow_data/training_data/training_images_batch\" + str(batch_number) + \".npy\")[image_numbers]\n",
    "    \n",
    "def load_training_labels(batch_number, image_numbers=[]):\n",
    "    \"\"\"\n",
    "    Loads the training data from files. It is possible to specify an interval \n",
    "    of images to load, or by defauly load th entire batch.\n",
    "    \"\"\"\n",
    "    if image_numbers == []:\n",
    "        return np.load(\"./TensorFlow_data/training_data/training_labels_batch\" + str(batch_number) + \".npy\")\n",
    "    else:\n",
    "        return np.load(\"./TensorFlow_data/training_data/training_labels_batch\" + str(batch_number) + \".npy\")[image_numbers]\n",
    "\n",
    "def display_image(imagearray):\n",
    "    array_to_plot = imagearray\n",
    "    print(\"Image shape: {}\".format(imagearray.shape))\n",
    "    plt.imshow(array_to_plot)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "display_image(load_training_data(0)[19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up functions necessary to build the neural network\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"    \n",
    "    # Number of input colors\n",
    "    num_inputcolors = x_tensor.shape.as_list()[3]\n",
    "    \n",
    "    # Convolutional filter\n",
    "    W_conv= tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], num_inputcolors, conv_num_outputs], stddev=0.1))\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[conv_num_outputs]))\n",
    "    \n",
    "    convolution = tf.nn.conv2d(x_tensor, W_conv, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    h_conv = tf.nn.relu(convolution + b_conv)\n",
    "    \n",
    "    h_pool = tf.nn.max_pool(h_conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                            strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    return h_pool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"    \n",
    "    flat_dimension = np.prod(x_tensor.shape.as_list()[1:])\n",
    "    x_flat = tf.reshape(x_tensor, [-1, flat_dimension])\n",
    "\n",
    "    return x_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"    \n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_connected = tf.nn.relu(tf.matmul(x_tensor, W) + b)\n",
    "    \n",
    "    return h_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_output = tf.matmul(x_tensor, W) + b \n",
    "    \n",
    "    return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convlayer_1 = tf.nn.dropout(conv2d_maxpool(x, 20, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_1b = tf.nn.dropout(conv2d_maxpool(convlayer_1, 10, (4, 4), (1, 1), (2, 2), (1, 1)), keep_prob)\n",
    "    \n",
    "    convlayer_2 = tf.nn.dropout(conv2d_maxpool(convlayer_1, 30, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_2b = conv2d_maxpool(convlayer_2, 20, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "    \n",
    "    #convlayer_3 = tf.nn.dropout(conv2d_maxpool(convlayer_2, 60, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_3b = conv2d_maxpool(convlayer_3, 50, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened_tensor = flatten(convlayer_2)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    connlayer_1 = tf.nn.dropout(fully_conn(flattened_tensor, 100), keep_prob) #200 earlier\n",
    "    \n",
    "    connlayer_2 = tf.nn.dropout(fully_conn(connlayer_1, 30), keep_prob)\n",
    "    \n",
    "    #connlayer_3 = tf.nn.dropout(fully_conn(connlayer_2, 30), keep_prob)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    outputlayer = output(connlayer_2, 3)\n",
    "\n",
    "    return outputlayer\n",
    "\n",
    "#=============================\n",
    "#  Build the Neural Network\n",
    "#=============================\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((150, 150, 3))\n",
    "y = neural_net_label_input(3)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0001 # default is 0.001 N.B. it is also possible to make this a placeholder object!\n",
    "size_of_minibatch = 2**6\n",
    "\n",
    "#===============================\n",
    "# Don't need to edit below this\n",
    "#===============================\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print cost and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tensorflow_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "training_folder = tensorflow_folder + training_subfolder\n",
    "num_saved_batches = sum([\"training_images_batch\" in filename \n",
    "                         for filename in list(os.walk(training_folder))[0][2]])\n",
    "\n",
    "# The final batch will be our validation set\n",
    "validation_inputarray = load_training_data(num_saved_batches - 1)\n",
    "validation_labels = load_training_labels(num_saved_batches - 1)\n",
    "\n",
    "def get_stats(session, feature_batch, label_batch, cost, accuracy, printout=True):\n",
    "    \"\"\"\n",
    "    Obtain information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    training_cost_value = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    validation_cost_value = session.run(cost, feed_dict={x: validation_inputarray,\n",
    "                                                         y: validation_labels, keep_prob:1.0})\n",
    "    accuracy_value = session.run(accuracy, feed_dict={x: validation_inputarray, \n",
    "                                                      y: validation_labels, keep_prob:1.0})\n",
    "    if printout:\n",
    "        print(\"\\nTraining Loss: {}\".format(training_cost_value))\n",
    "        print(\"Validation Loss: {}\".format(validation_cost_value))\n",
    "        print(\"Accuracy (validation): {}\".format(accuracy_value))\n",
    "    return training_cost_value, validation_cost_value, accuracy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_list(inputlist, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the inputlist split into batches of maximal length batch_size.\n",
    "    Each element in the returned list (i.e. each batch) is itself a list.\n",
    "    \"\"\"\n",
    "    list_of_batches = [inputlist[ii: ii+batch_size] for ii in range(0, len(inputlist), batch_size)]\n",
    "    return list_of_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a folder into which we place the trained Neural Networks models\n",
    "foldername_trainedmmodels = \"./TensorFlow_data/trained_models\"\n",
    "if not os.path.exists(foldername_trainedmmodels):\n",
    "    os.makedirs(foldername_trainedmmodels)\n",
    "    \n",
    "currentmodel_name = \"lenet2x2\" # This means we have 2 convolutional layers and 2 fully connected layers\n",
    "savedmodel_path = foldername_trainedmmodels + \"/\" + currentmodel_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which of the training or testing cells below to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on a single batch\n",
    "\n",
    "In order to pick the best hyperparameters, we begin by training on a single batch. This will tell us when to stop the learning and will help in choosing a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "\n",
    "print('Checking the Training on a Single Batch, i.e. number {}'.format(batch_i))\n",
    "\n",
    "accuracy_list = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), size_of_minibatch),\n",
    "                                                   batch_list(load_training_labels(batch_i), size_of_minibatch)):\n",
    "            sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "        (training_cost_value, \n",
    "         validation_cost_value, \n",
    "         accuracy_value) = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                     printout=True)\n",
    "        print('\\nEpoch {:>2}, Batch {}: {} '.format(epoch + 1, batch_i, accuracy_value))\n",
    "        training_losses.append(training_cost_value)\n",
    "        validation_losses.append(validation_cost_value)\n",
    "        accuracy_list.append(accuracy_value)\n",
    "    \n",
    "    # Save the model\n",
    "    #saver = tf.train.Saver()\n",
    "    #save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "accuracy_list = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "with tf.Session() as sess:\n",
    "    # It is very important the saver is defined INSIDE the block \"with tf.Session() as sess\"\n",
    "    # otherwise it will be very difficult to load the graph (unless we name all the variables etc)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "            (training_cost_value, \n",
    "             validation_cost_value, \n",
    "             accuracy_value) = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                         printout=True)\n",
    "            \n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        training_losses.append(training_cost_value)\n",
    "        validation_losses.append(validation_cost_value)\n",
    "        accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            save_path = saver.save(sess, savedmodel_path, global_step=epoch)\n",
    "    \n",
    "    # Save the final model\n",
    "    save_path = saver.save(sess, savedmodel_path, global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decrease the learning rate for the final part!\n",
    "\n",
    "epochs = 50\n",
    "load_model = savedmodel_path + \"-70\"\n",
    "\n",
    "# read off the epoch from the number in load_model\n",
    "next_epoch = int(load_model[load_model.rfind(\"-\")+1:]) + 1\n",
    "\n",
    "accuracy_list = []\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, load_model)\n",
    "    print(sess.run(accuracy, feed_dict={x: validation_inputarray,\n",
    "                                        y: validation_labels, keep_prob:1.0}))\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(next_epoch, next_epoch + epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, \n",
    "                                                  keep_prob: keep_probability})\n",
    "            \n",
    "            (training_cost_value, \n",
    "             validation_cost_value, \n",
    "             accuracy_value) = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                         printout=True)\n",
    "            \n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        training_losses.append(training_cost_value)\n",
    "        validation_losses.append(validation_cost_value)\n",
    "        accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            ###save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "            save_path = saver.save(sess, savedmodel_path, global_step=epoch)\n",
    "            \n",
    "    # Save the final model\n",
    "    ###save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "    save_path = saver.save(sess, savedmodel_path, global_step=epoch)\n",
    "    \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model = savedmodel_path + \"-70\"\n",
    "testing_inputarray = np.load(\"./TensorFlow_data/testing_data/testing_images.npy\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, load_model)\n",
    "    \n",
    "    # We need to supply a y. Since it plays no role when predicting probabilities, we supply an \n",
    "    # empty matrix with the right dimensions.\n",
    "    empty_y = np.zeros((testing_inputarray.shape[0], testing_inputarray.shape[-1]))\n",
    "    probabilities = sess.run(tf.nn.softmax(logits), feed_dict={x: testing_inputarray, y: empty_y, keep_prob:1.0})\n",
    "\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission_folder = \"./submissions\"\n",
    "if not os.path.exists(submission_folder):\n",
    "    os.makedirs(submission_folder)\n",
    "    \n",
    "\n",
    "def image_index(pathname):\n",
    "    return pathname[pathname.rfind(\"/\")+1:]\n",
    "\n",
    "image_names = np.load(\"./TensorFlow_data/testing_data/testing_namelabels.npy\")\n",
    "categories = np.load(\"./TensorFlow_data/testing_data/type123_order.npy\")\n",
    "\n",
    "image_names = [image_index(path) for path in image_names]\n",
    "\n",
    "submission_df = pd.DataFrame(probabilities, columns=categories)\n",
    "submission_df[\"image_name\"] = image_names\n",
    "submission_df.set_index(\"image_name\", inplace=True)\n",
    "\n",
    "def get_date_string():\n",
    "    currentime = pd.datetime.now().isoformat()\n",
    "    # We will turn this into the format \"yyyy-mm-dd_hh:mm\"\n",
    "    dateandminute = currentime[:currentime.rfind(\":\")].replace(\"T\", \"_\").replace(\":\", \"-\")\n",
    "    return dateandminute\n",
    "\n",
    "filename = submission_folder + \"/submissions_\" + get_date_string() + \".csv\"\n",
    "submission_df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
