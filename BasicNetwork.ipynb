{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Neural Network\n",
    "\n",
    "This code significantly overlaps with the code from Project 5: Image Classification, in another [GitHub folder](https://github.com/dangall/Udacity-Machine-Learning-Nanodegree/blob/master/P5_image_classification/image_classification.ipynb).\n",
    "\n",
    "### Set up functions necessary to build the neural network\n",
    "\n",
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, image_shape[0], image_shape[1], image_shape[2]], name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, [None, n_classes], name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"    \n",
    "    # Number of input colors\n",
    "    num_inputcolors = x_tensor.shape.as_list()[3]\n",
    "    \n",
    "    # Convolutional filter\n",
    "    W_conv= tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], num_inputcolors, conv_num_outputs], stddev=0.1))\n",
    "    b_conv = tf.Variable(tf.constant(0.1, shape=[conv_num_outputs]))\n",
    "    \n",
    "    convolution = tf.nn.conv2d(x_tensor, W_conv, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME')\n",
    "    h_conv = tf.nn.relu(convolution + b_conv)\n",
    "    \n",
    "    h_pool = tf.nn.max_pool(h_conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], \n",
    "                            strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "    \n",
    "    return h_pool "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"    \n",
    "    flat_dimension = np.prod(x_tensor.shape.as_list()[1:])\n",
    "    x_flat = tf.reshape(x_tensor, [-1, flat_dimension])\n",
    "\n",
    "    return x_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"    \n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_connected = tf.nn.relu(tf.matmul(x_tensor, W) + b)\n",
    "    \n",
    "    return h_connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    input_dimensions = x_tensor.shape.as_list()[1]    \n",
    "    W = tf.Variable(tf.truncated_normal([input_dimensions, num_outputs], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_outputs]))\n",
    "    \n",
    "    h_output = tf.matmul(x_tensor, W) + b\n",
    "    \n",
    "    return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    convlayer_1 = tf.nn.dropout(conv2d_maxpool(x, 20, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_1b = tf.nn.dropout(conv2d_maxpool(convlayer_1, 10, (4, 4), (1, 1), (2, 2), (1, 1)), keep_prob)\n",
    "    \n",
    "    convlayer_2 = tf.nn.dropout(conv2d_maxpool(convlayer_1, 30, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_2b = conv2d_maxpool(convlayer_2, 20, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "    \n",
    "    convlayer_3 = tf.nn.dropout(conv2d_maxpool(convlayer_2, 60, (4, 4), (1, 1), (2, 2), (2, 2)), keep_prob)\n",
    "    #convlayer_3b = conv2d_maxpool(convlayer_3, 50, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flattened_tensor = flatten(convlayer_3)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    connlayer_1 = tf.nn.dropout(fully_conn(flattened_tensor, 200), keep_prob)\n",
    "    \n",
    "    connlayer_2 = tf.nn.dropout(fully_conn(connlayer_1, 100), keep_prob)\n",
    "    \n",
    "    connlayer_3 = tf.nn.dropout(fully_conn(connlayer_2, 30), keep_prob)\n",
    "    \n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    outputlayer = output(connlayer_3, 10)\n",
    "\n",
    "    return outputlayer\n",
    "\n",
    "#=============================\n",
    "#  Build the Neural Network\n",
    "#=============================\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((28, 28, 1))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "#logits = conv_net(x, keep_prob)\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "keep_probability = 0.5\n",
    "learning_rate = 0.0001 # default is 0.001 N.B. it is also possible to make this a placeholder object!\n",
    "size_of_minibatch = 2**7\n",
    "\n",
    "#===============================\n",
    "# Don't need to edit below this\n",
    "#===============================\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print cost and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The final batch will be our validation set\n",
    "validation_inputarray = load_training_data(num_saved_batches - 1)\n",
    "validation_labels = load_training_labels(num_saved_batches - 1)\n",
    "\n",
    "def get_stats(session, feature_batch, label_batch, cost, accuracy, printout=True):\n",
    "    \"\"\"\n",
    "    Obtain information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    cost_value = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob:1.0})\n",
    "    accuracy_value = session.run(accuracy, feed_dict={x: validation_inputarray, \n",
    "                                                      y: validation_labels, keep_prob:1.0})\n",
    "    if printout:\n",
    "        print(\"\\nLoss: {}\".format(cost_value))\n",
    "        print(\"Accuracy (validation): {}\".format(accuracy_value))\n",
    "    return cost_value, accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose which of the training or testing cells below to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on a single batch\n",
    "\n",
    "In order to pick the best hyperparameters, we begin by training on a single batch. This will tell us when to stop the learning and will help in choosing a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_i = 0\n",
    "\n",
    "print('Checking the Training on a Single Batch, i.e. number {}'.format(batch_i))\n",
    "\n",
    "accuracy_list = []\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), size_of_minibatch),\n",
    "                                                   batch_list(load_training_labels(batch_i), size_of_minibatch)):\n",
    "            sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "        cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                               printout=False)\n",
    "        #print('\\nEpoch {:>2}, Batch {}: {} '.format(epoch + 1, batch_i, accuracy_value), end='')\n",
    "        accuracy_list.append(accuracy_value)\n",
    "    \n",
    "    # Save the model\n",
    "    #saver = tf.train.Saver()\n",
    "    #save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "accuracy_list = []\n",
    "with tf.Session() as sess:\n",
    "    # It is very important the saver is defined INSIDE the block \"with tf.Session() as sess\"\n",
    "    # otherwise it will be very difficult to load the graph (unless we name all the variables etc)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, keep_prob: keep_probability})\n",
    "            cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy, \n",
    "                                                   printout=False)\n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "    \n",
    "    # Save the final model\n",
    "    save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "        \n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decrease the learning rate for the final part!\n",
    "\n",
    "epochs = 30\n",
    "load_model = \"./trained_model-29\"\n",
    "\n",
    "# read off the epoch from the number in load_model\n",
    "next_epoch = int(load_model[load_model.rfind(\"-\")+1:]) + 1\n",
    "\n",
    "new_accuracy_list = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, load_model)\n",
    "    print(sess.run(accuracy, feed_dict={x: validation_inputarray,\n",
    "                                        y: validation_labels, keep_prob:1.0}))\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(next_epoch, next_epoch + epochs):\n",
    "        for batch_i in range(num_saved_batches - 1):\n",
    "            for batch_inputarrays, batch_labels in zip(batch_list(load_training_data(batch_i), \n",
    "                                                                  size_of_minibatch),\n",
    "                                                       batch_list(load_training_labels(batch_i), \n",
    "                                                                  size_of_minibatch)\n",
    "                                                      ):\n",
    "                sess.run(optimizer, feed_dict={x: batch_inputarrays, y: batch_labels, \n",
    "                                                  keep_prob: keep_probability})\n",
    "            cost_value, accuracy_value = get_stats(sess, batch_inputarrays, batch_labels, cost, accuracy,\n",
    "                                                   printout=False)\n",
    "            print('Epoch {:>2}, Batch {}: {}'.format(epoch + 1, batch_i, accuracy_value))\n",
    "        \n",
    "        new_accuracy_list.append(accuracy_value)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            # Save the intermediate model\n",
    "            save_path = saver.save(sess, \"./trained_model\", global_step=epoch)\n",
    "            \n",
    "    # Save the final model\n",
    "    save_path = saver.save(sess, \"./trained_model\", global_step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(new_accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model = \"./trained_model-59\"\n",
    "testing_inputarray = np.load(\"./testing_data/testing_images.npy\")\n",
    "testing_labels = np.load(\"./testing_data/testing_labels.npy\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    saver.restore(sess, load_model)\n",
    "    print(\"The model's test-set accuracty is {}%\".format(np.round(sess.run(accuracy, \n",
    "                                                                            feed_dict={x: testing_inputarray,\n",
    "                                        y: testing_labels, keep_prob:1.0})*100, decimals=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
