{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tarfile\n",
    "from os.path import isdir, isfile\n",
    "from os import remove\n",
    "\n",
    "import scipy.ndimage\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find path names of files and prepare one-hot encoder\n",
    "\n",
    "We begin by listing all image-file paths to prepare for training. We also create a one-hot encoder for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_folders = [\"./Data/train\"] #+ [\"./Data/Type_1\", \"./Data/Type_2\", \"./Data/Type_3\"]\n",
    "testing_folder = \"./Data/test\"\n",
    "\n",
    "def all_image_paths(folderpath):\n",
    "    \"\"\"\n",
    "    Returns a list of filenames containing 'jpg'. The returned list has sublists with filenames,\n",
    "    where each sublist is a different folder.\n",
    "    \"\"\"\n",
    "    image_pathnames = [[folderandfiles[0]+\"/\"+imname for imname in folderandfiles[2] if \"jpg\" in imname] \n",
    "                          for folderandfiles in os.walk(folderpath) if folderandfiles[2]!=[]]\n",
    "    image_pathnames = [folder for folder in image_pathnames if folder != []]\n",
    "    return image_pathnames\n",
    "\n",
    "\n",
    "# We first get all path-names for the training and testing images\n",
    "training_pathnames = sum([all_image_paths(folder) for folder in training_folders], [])\n",
    "testing_pathnames = all_image_paths(testing_folder)\n",
    "\n",
    "def get_Type(filepath): #formerly get_letter\n",
    "    \"\"\"\n",
    "    Returns the type corresponding to an image found in filepath\n",
    "    \"\"\"\n",
    "    # The type number is given by the name of the folder in which we find the image\n",
    "    indexname = filepath.rfind(\"/\")\n",
    "    letter = filepath[indexname-6:indexname]\n",
    "    return letter\n",
    "\n",
    "# In each folder all images depict the same cervical type\n",
    "all_Types = np.sort([get_Type(pathname[0]) for pathname in training_pathnames])\n",
    "\n",
    "# We may now make the function that one-hot-encodes Types into arrays\n",
    "enc = LabelBinarizer()\n",
    "enc.fit(all_Types)\n",
    "\n",
    "def one_hot_encode(list_of_types):\n",
    "    \"\"\"\n",
    "    One hot encode a list of Types. Returns a one-hot encoded vector for each Type.\n",
    "    \"\"\"\n",
    "    return enc.transform(list_of_types)\n",
    "\n",
    "# We now flatten the lists of path names\n",
    "training_pathnames = np.array(sum(training_pathnames, []))\n",
    "testing_pathnames = np.array(sum(testing_pathnames, []))\n",
    "\n",
    "# When training, we don't want the images to be ordered. Therefore, we take a \n",
    "# random permutation of their order.\n",
    "np.random.seed(42)\n",
    "training_pathnames = np.random.permutation(training_pathnames)\n",
    "print(\"We are training on {} images\".format(len(training_pathnames)))\n",
    "print(\"We are testing on {} images\".format(len(testing_pathnames)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load images and labels into arrays, save to disk\n",
    "\n",
    "We first normalize the pixel-values to lie between 0 and 1. We also reshape each image to be a 3-dimensional array: (x_length, y_length, color_channels). \n",
    "\n",
    "Finally, we save the arrays to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose an appropriate resizing of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The insput is a list of matrices describing the pixels. The matrices are not required to all have the same resolution.\n",
    "def drawSlices(list_of_image_matrices, figsizes=(16,4)):\n",
    "    # We'll make a figure with subplots in it\n",
    "    numberofrows = int(np.ceil(len(list_of_image_matrices) / 6.))\n",
    "    fig, axes = plt.subplots(nrows=numberofrows, ncols=6, figsize=(figsizes[0],figsizes[1]*numberofrows))\n",
    "    # Now on each axis we can draw the slice\n",
    "    for (currentax, currentimage) in zip(axes.ravel(), list_of_image_matrices):\n",
    "        currentax.imshow(currentimage, cmap=\"gray\")\n",
    "        # The ticks are useless and ugly\n",
    "        currentax.set_xticks([])\n",
    "        currentax.set_yticks([])\n",
    "    # Finally we remove those plots that have nothing in them\n",
    "    for remainingax in axes.ravel()[len(list_of_image_matrices):]:\n",
    "        remainingax.axis(\"off\")\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drawSlices([scipy.ndimage.imread(impath) for impath in training_pathnames[7:13]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resize_shape = (150, 150, 3)\n",
    "\n",
    "imagearray = scipy.ndimage.imread(training_pathnames[17])\n",
    "resized_imagearray = scipy.misc.imresize(imagearray, resize_shape)\n",
    "drawSlices([imagearray, resized_imagearray]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "150 to 300 pixels on the x and y axes seems like an appropriate resizing, where we don't lose too much releveant info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_normalize_image(path, resize_shape=(150, 150, 3)):\n",
    "    \"\"\"\n",
    "    Takes the directory path of an image and returns a normalized\n",
    "    3-dimensional array representing that image.\n",
    "    \"\"\"\n",
    "    # First we load the image\n",
    "    try:\n",
    "        imagearray = scipy.ndimage.imread(path)\n",
    "        # There is no need to reshape the image to be three-dimensional; they already are.\n",
    "        # We do want to resize it however.\n",
    "        imagearray = scipy.misc.imresize(imagearray, resize_shape)\n",
    "        # Now we normalize it \n",
    "        imagearray = imagearray / 255.\n",
    "        return imagearray\n",
    "    except:\n",
    "        # If some images are broken in the database; these will raise errors.\n",
    "        pass\n",
    "    \n",
    "def array_all_images(list_of_path_names, parallelize=False):\n",
    "    \"\"\"\n",
    "    Takes a list of directory paths of images and returns a 4-dimensional array\n",
    "    containing the pixel-data of those images. The shape is:\n",
    "    (num_images, x_dim, y_dim, num_colors)\n",
    "    \"\"\"\n",
    "    if parallelize:\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        all_images = Parallel(n_jobs=num_cores)(delayed(load_normalize_image)(path) \n",
    "                                               for path in list_of_path_names)\n",
    "    else:\n",
    "        all_images = [load_normalize_image(path) for path in list_of_path_names]\n",
    "    # Some of these might be None since the function load_normalize_image\n",
    "    # does not load broken images. We now remove these Nones.\n",
    "    all_images = [img for img in all_images if img is not None] # IN PYTHON 3 np.array(list(filter(None.__ne__, all_images)))\n",
    "    return all_images\n",
    "\n",
    "def load_Type(path): #formerly load_letter\n",
    "    \"\"\"\n",
    "    Takes the directory path of an image and returns the label of the image.\n",
    "    \"\"\"\n",
    "    # First we see if it is possible to load the image\n",
    "    try:\n",
    "        imagearray = scipy.ndimage.imread(path)\n",
    "        imagearray = scipy.misc.imresize(imagearray, resize_shape)\n",
    "        # If this didn't give an error, we may get the letter\n",
    "        return get_Type(path)\n",
    "    except:\n",
    "        # Some images are broken in the database; these will raise errors.\n",
    "        pass\n",
    "\n",
    "def array_all_labels(list_of_path_names, parallelize=False):\n",
    "    \"\"\"\n",
    "    Takes a list of directory paths of images and returns a 2-dimensional array\n",
    "    containing the one-hot-encoded labels of those images\n",
    "    \"\"\"\n",
    "    if parallelize:\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        the_types = Parallel(n_jobs=num_cores)(delayed(load_Type)(path) for path in list_of_path_names)\n",
    "    else:\n",
    "        the_types = [load_Type(path) for path in list_of_path_names]\n",
    "    the_types = [typ for typ in the_types if typ is not None] # IN PYTHON 3: list(filter(None.__ne__, the_types))\n",
    "    all_labels = one_hot_encode(the_types)\n",
    "    return all_labels\n",
    "\n",
    "def batch_list(inputlist, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the inputlist split into batches of maximal length batch_size.\n",
    "    Each element in the returned list (i.e. each batch) is itself a list.\n",
    "    \"\"\"\n",
    "    list_of_batches = [inputlist[ii: ii+batch_size] for ii in range(0, len(inputlist), batch_size)]\n",
    "    return list_of_batches\n",
    "\n",
    "tensorflow_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "\n",
    "# We store all the data in a training and testing folder\n",
    "training_folder = tensorflow_folder + training_subfolder\n",
    "testing_folder = tensorflow_folder + testing_subfolder\n",
    "if not os.path.exists(training_folder):\n",
    "    os.makedirs(training_folder)\n",
    "if not os.path.exists(testing_folder):\n",
    "    os.makedirs(testing_folder)\n",
    "\n",
    "# Make the input data and labels for the testing set\n",
    "testingarrayimage_path = \"/testing_images.npy\"\n",
    "if isfile(testing_folder + testingarrayimage_path) == False:\n",
    "    testing_images = array_all_images(testing_pathnames, parallelize=True)\n",
    "    np.save(testing_folder + testingarrayimage_path, testing_images)\n",
    "\n",
    "# Here we specify the size of each batch\n",
    "batch_size = 2**7\n",
    "\n",
    "# Now we save the batch-data, unless it already exists\n",
    "training_pathnames_batches = batch_list(training_pathnames, batch_size)\n",
    "num_saved_batches = sum([\"training_images_batch\" in filename \n",
    "                         for filename in list(os.walk(training_folder))[0][2]])\n",
    "\n",
    "# If we have a different number of batches saved comapred to what we want,\n",
    "# the batches are wrong and need recomputing.\n",
    "if num_saved_batches != len(training_pathnames_batches):\n",
    "    # We could delete the old files, but this is dangerous, since a \n",
    "    # typo could remove all files on the computer. We simply overwrite the files we have\n",
    "    for ii, batch in enumerate(tqdm(training_pathnames_batches)):\n",
    "        training_images_batch = array_all_images(batch, parallelize=True)\n",
    "        np.save(training_folder + \"/training_images_batch\" + str(ii) + \".npy\", training_images_batch)\n",
    "\n",
    "        training_labels_batch = array_all_labels(batch, parallelize=True)\n",
    "        np.save(training_folder + \"/training_labels_batch\" + str(ii) + \".npy\", training_labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
