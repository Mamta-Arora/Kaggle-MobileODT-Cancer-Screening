{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Here we preprocess the data, loading the image files and saving them to disk them as numpy arrays.\n",
    "\n",
    "We can specify the folder structure of how we save the files, which images we import for training and testing, and what type of preprocessing we choose to do on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from basic_preprocessing import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify which folders to read from when loading training and testing data\n",
    "\n",
    "Each training image must be placed in a folder whose name is the label for the image. For example, a Type 1 image called `1.jpg` must have a path ending in `\"/Type_1/1.jpg\"`.\n",
    "\n",
    "The testing images, on the other hand, have no labels and should thus be located all together in a folder.\n",
    "\n",
    " - `training_folders` should be a list of folder-names we get images from, i.e. a list of strings.\n",
    " - `testing_folder` should be a single folder, i.e. a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "training_folders = [\"./Data/train\"]\n",
    "# + [\"./Data/Type_1\", \"./Data/Type_2\", \"./Data/Type_3\"]\n",
    "testing_folder = \"./Data/test\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a preprocessing object and test whether it reads the images correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc = DataPreprocessor(training_folders=training_folders, testing_folder=testing_folder)\n",
    "preproc.test_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a good image resizing\n",
    "\n",
    "The input images are large, typically several thousands of pixels on each axis. We want to resize them to as small as possible, while allowing a human to look at the image and still be able to tell which Type it is.\n",
    "\n",
    "Here we can play with the variable `resize_shape` to choose one that does not wash out important information in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc.test_resizing(resize_shape=(150, 150, 3), index_image=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters for preprocessing\n",
    "\n",
    " - `data_folder` is the folder in which we save the preprocessed numpy arrays, needed for TensorFlow training.\n",
    " - `training_subfolder` is the subfolder inside `data_folder` which contains the training numpy arrays (and their labels).\n",
    " - `testing_subfolder` is the subfolder inside `data_folder` which contains the testing numpy arrays (as well as a little auxiliary information about the arrays: which image each row corresponds to, and what order the Type 1, Type 2 and Type 3 appear in the one-hot-encoded labels).\n",
    " - `optimal_resize_shape` is the best resizing shape chosen above.\n",
    " - There are a lot of training images. We thus preprocess them in batches. `batch_size` controls the number of images in each batch. Note that if some images cannot be preprocessed, some batches will contain fewer than `batch_size` images.\n",
    " - Preprocessing can take some time. `parallelize` controls whether we want to parallelize this job over all local cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "optimal_resize_shape = (150, 150, 3)\n",
    "batch_size = 2**7\n",
    "parallelize = True\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the preprocessing\n",
    "\n",
    "*N.B. The preprocessing only gets recomputed if it is unable to detect the correct number of files in the expected folders. Therefore, if you change `optimal_resize_shape` and leave all other variables intact, the preprocessing will not happen! If you want to make sure all the numpy arrays get recomptued, you should delete the folders containing the numpy-array data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc.preprocess_save(data_folder=data_folder,\n",
    "                        training_subfolder=training_subfolder,\n",
    "                        testing_subfolder=testing_subfolder,\n",
    "                        resize_shape=optimal_resize_shape, batch_size=batch_size,\n",
    "                        parallelize=parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Still need to include image flips!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We construct a variety of neural networks and study their behavior to the amount and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convnet import ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LetNet-like networks\n",
    "\n",
    "We will first study simple LeNet-like convolutional neural networks, which consist of a couple of convolutional layers (with max pooling) followed by a few fully-connected layers.\n",
    "\n",
    "After constructing a simple network, we'll begin by checking how the validation performance of the network is affected by the quantitity of training data, to determine whether the network needs to be more complex or whether we need to increase the amount of data we train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters\n",
    "\n",
    "We will train on a network with a specific architecture. To speed up training, we will train with a larger learning rate, and fine-tune our training with a smaller learning rate.\n",
    "\n",
    " - `input_shape` is the dimension-shape of the input image arrays.\n",
    " - `output_channels` is the number of output categories (i.e. cervix Types).\n",
    " - `convolutional_layers` specifies the parameters for the convolutional + max pooling layers. Each layer has the form  \n",
    " ```[int (number of output channels),  \n",
    "    tuple of length 2 (size of conv filter),  \n",
    "    tuple of length 2 (step size of conv filter),  \n",
    "    tuple of length 2 (size of max pooling filter),  \n",
    "    tuple of length 2 (step size max pooling filter)]```\n",
    " - `connected_layers` specifies the parameters for the fully connected layers. It is a list where each element is the number of neurons in the layer.\n",
    " - `keep_prob` is the dropout keep_prob (usually recommended to be 0.5).\n",
    " - `learning_rate`is the learning rate. We can change this later with the function `set_learning_rate`.\n",
    " - `model_name` is the model's name, used when saving and loading trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "input_shape = (150, 150, 3)\n",
    "output_channels = 3\n",
    "convolutional_layers = [[20, (4, 4), (1, 1), (2, 2), (2, 2)],\n",
    "                        [30, (4, 4), (1, 1), (2, 2), (2, 2)]]\n",
    "connected_layers = [100, 30]\n",
    "keep_prob = 0.5\n",
    "learning_rate = 0.001 # standard value is 0.001\n",
    "model_name = \"testmodel\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the network with the properties specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "simple_network = ConvNet(input_shape=input_shape,\n",
    "                         output_channels=output_channels, \n",
    "                         convolutional_layers=convolutional_layers,\n",
    "                         connected_layers=connected_layers, \n",
    "                         keep_prob=keep_prob,\n",
    "                         learning_rate=learning_rate,\n",
    "                         model_name=model_name) \n",
    "                         # This name means we have 2 convolutional layers and 2 fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now train the neural network. We begin by specifying the parameters for training.\n",
    "\n",
    " - `data_folder` is the folder in which we save the preprocessed numpy arrays, needed for TensorFlow training.\n",
    " - `training_subfolder` is the subfolder inside `data_folder` which contains the training numpy arrays (and their labels).\n",
    " - `testing_subfolder` is the subfolder inside `data_folder` which contains the testing numpy arrays (as well as a little auxiliary information about the arrays: which image each row corresponds to, and what order the Type 1, Type 2 and Type 3 appear in the one-hot-encoded labels).\n",
    " - `saved_model_subfolder` is the subfolder inside `data_folder` in which we will store our trained models.\n",
    " - `size_of_minibatch` is the size of each training-step minibatch, i.e. the number of input images in each traininig step\n",
    "\n",
    "The validation set can be specified in one of two ways. Either we can specify a batch number to be validation set, or we can feed it a specific array of our choice.\n",
    "\n",
    " - `validation_batch` is the integer specifying which validation batch we use for validation.\n",
    " - Alternatively, we can specify `validation_inputarray` as the input array (with shape `(number of data points, size of x-axis, size of y-axis, number of color channels)`). We then also need to specify the validation labels with `validation_labels` (which should have shape `(number of data points, number of output channels)`).\n",
    " \n",
    "***N.B. It's important you ONLY set `validation_batch`, or ONLY set `validation_inputarray` and `validation_labels`!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "saved_model_subfolder = \"/trained_models\"\n",
    "size_of_minibatch = 2**6\n",
    "validation_batch = simple_network.count_training_batches(data_folder + training_subfolder) - 1\n",
    "#validation_inputarray = load_training_data(validation_batch)\n",
    "#validation_labels = load_training_labels(validation_batch)\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether the network correctly loads the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "simple_network.test_loading(batch_and_index=(0, 19), batch_loc=(data_folder+training_subfolder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit the network on a single batch\n",
    "\n",
    "To choose good parameters for training, it helps to first try and overfit the network on a batch. This will also help us check if everything is working correctly.\n",
    "\n",
    "Each time we train we specify:\n",
    "\n",
    " - `epochs` is the number of epochs we train for.\n",
    " - `load_saved_model` is used for loading a saved model. It is the full path to the saved model, including epoch number, e.g. `./TensorFlow_data/trained_models/mysavedmodel-20`. If we do not want to load a model, we set `load_saved_model=\"\"`.\n",
    " - `training_batches` is a list speciying the set of batches we use for training, i.e. each element of `training_batches` is an integer.\n",
    " - `save_model` is a boolean specifying whether we should save our trained model as we train. The ConvNet saves the model every ten epochs, as well as the final epoch.\n",
    " - `model_destination_folder` is the folder in which we want to save our trained model. If we are loading a saved model and further training it, this variable can be set to `model_destination_folder=\"\"`, which will save the further trained models into the same folder as `load_saved_model`. \n",
    " - `printout` is a boolean which sets whether we want printouts with loss and accuracy as we train.\n",
    " - We set the learning rate for this training run with the `set_learning_rate` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "epochs = 45\n",
    "load_saved_model = \"./TensorFlow_data/deleteme_cont/testmodel-90\"\n",
    "training_batches = [1, 2]\n",
    "save_model = True\n",
    "model_destination_folder = \"\"\n",
    "printout = True\n",
    "simple_network.learning_rate = 0.0002\n",
    "simple_network.keep_prob = 0.75\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "#Work out whether we specify the validation set using a batch number or a manually set array\n",
    "try:\n",
    "    validation_batch\n",
    "    validation_inputarray = []\n",
    "    validation_labels = []\n",
    "except:\n",
    "    validation_batch=0\n",
    "\n",
    "# Perform the training\n",
    "(accuracy_list,\n",
    "training_losses,\n",
    "validation_losses) = simple_network.train(epochs=epochs,\n",
    "                                          load_saved_model=load_saved_model,\n",
    "                                          training_batches=training_batches,\n",
    "                                          size_of_minibatch=size_of_minibatch,\n",
    "                                          validation_inputarray=validation_inputarray,\n",
    "                                          validation_labels=validation_labels,\n",
    "                                          validation_batchnum=validation_batch,\n",
    "                                          printout=printout,\n",
    "                                          save_model=save_model,\n",
    "                                          model_destination_folder=model_destination_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_losses[2:])\n",
    "plt.plot(validation_losses[2:])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
