{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Here we preprocess the data, loading the image files and saving them to disk them as numpy arrays.\n",
    "\n",
    "We can specify the folder structure of how we save the files, which images we import for training and testing, and what type of preprocessing we choose to do on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from basic_preprocessing import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify which folders to read from when loading training and testing data\n",
    "\n",
    "Each training image must be placed in a folder whose name is the label for the image. For example, a Type 1 image called `1.jpg` must have a path ending in `\"/Type_1/1.jpg\"`.\n",
    "\n",
    "The testing images, on the other hand, have no labels and should thus be located all together in a folder.\n",
    "\n",
    " - `training_folders` should be a list of folder-names we get images from, i.e. a list of strings.\n",
    " - `testing_folder` should be a single folder, i.e. a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "training_folders = [\"./Data/train\"]\n",
    "# + [\"./Data/Type_1\", \"./Data/Type_2\", \"./Data/Type_3\"]\n",
    "testing_folder = \"./Data/test\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a preprocessing object and test whether it reads the images correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc = DataPreprocessor(training_folders=training_folders, testing_folder=testing_folder)\n",
    "preproc.test_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a good image resizing\n",
    "\n",
    "The input images are large, typically several thousands of pixels on each axis. We want to resize them to as small as possible, while allowing a human to look at the image and still be able to tell which Type it is.\n",
    "\n",
    "Here we can play with the variable `resize_shape` to choose one that does not wash out important information in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc.test_resizing(resize_shape=(150, 150, 3), index_image=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters for preprocessing\n",
    "\n",
    " - `data_folder` is the folder in which we save the preprocessed numpy arrays, needed for TensorFlow training.\n",
    " - `training_subfolder` is the subfolder inside `data_folder` which contains the training numpy arrays (and their labels).\n",
    " - `testing_subfolder` is the subfolder inside `data_folder` which contains the testing numpy arrays (as well as a little auxiliary information about the arrays: which image each row corresponds to, and what order the Type 1, Type 2 and Type 3 appear in the one-hot-encoded labels).\n",
    " - `optimal_resize_shape` is the best resizing shape chosen above.\n",
    " - There are a lot of training images. We thus preprocess them in batches. `batch_size` controls the number of images in each batch. Note that if some images cannot be preprocessed, some batches will contain fewer than `batch_size` images.\n",
    " - Preprocessing can take some time. `parallelize` controls whether we want to parallelize this job over all local cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "optimal_resize_shape = (150, 150, 3)\n",
    "batch_size = 2**7\n",
    "parallelize = True\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the preprocessing\n",
    "\n",
    "*N.B. The preprocessing only gets recomputed if it is unable to detect the correct number of files in the expected folders. Therefore, if you change `optimal_resize_shape` and leave all other variables intact, the preprocessing will not happen! If you want to make sure all the numpy arrays get recomptued, you should delete the folders containing the numpy-array data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc.preprocess_save(data_folder=data_folder,\n",
    "                        training_subfolder=training_subfolder,\n",
    "                        testing_subfolder=testing_subfolder,\n",
    "                        resize_shape=optimal_resize_shape, batch_size=batch_size,\n",
    "                        parallelize=parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Still need to include image flips!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We construct a variety of neural networks and study their behavior to the amount and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from convnet import ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_network = ConvNet(input_shape=(150, 150, 3),\n",
    "                         output_channels=3, \n",
    "                         convolutional_layers=[[20, (4, 4), (1, 1), (2, 2), (2, 2)],\n",
    "                                               [30, (4, 4), (1, 1), (2, 2), (2, 2)]],\n",
    "                         connected_layers=[100, 30], \n",
    "                         keep_prob=1.0,\n",
    "                         learning_rate=0.002,\n",
    "                         model_name=\"testmodel\") \n",
    "                         # This name means we have 2 convolutional layers and 2 fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_network.test_loading(batch_and_index=(0, 19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "saved_model_subfolder = \"/trained_models\"\n",
    "size_of_minibatch = 2**6\n",
    "validation_batch = simple_network.count_training_batches(data_folder + training_subfolder) - 1\n",
    "#validation_inputarray = load_training_data(validation_batch)\n",
    "#validation_labels = load_training_labels(validation_batch)\n",
    "epochs = 50\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FIRST TRAIN ON A SINGLE BATCH\n",
    "(accuracy_list,\n",
    " training_losses,\n",
    " validation_losses) = simple_network.train(epochs=epochs, training_batches=[0], size_of_minibatch=2**6,\n",
    "                                           validation_batchnum=validation_batch, printout=True, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(accuracy_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(training_losses)\n",
    "plt.plot(validation_losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train(self, epochs=10, load_saved_model=\"\", training_batches=[],\n",
    "#              size_of_minibatch=2**6, validation_inputarray=[],\n",
    "#              validation_labels=[], validation_batchnum=0, printout=True,\n",
    "#              save_model=True, model_destination_folder=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
