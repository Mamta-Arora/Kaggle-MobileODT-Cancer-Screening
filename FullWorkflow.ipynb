{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Here we preprocess the data, loading the image files and saving them to disk them as numpy arrays.\n",
    "\n",
    "We can specify the folder structure of how we save the files, which images we import for training and testing, and what type of preprocessing we choose to do on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from basic_preprocessing import DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify which folders to read from when loading training and testing data\n",
    "\n",
    "Each training image must be placed in a folder whose name is the label for the image. For example, a Type 1 image called `1.jpg` must have a path ending in `\"/Type_1/1.jpg\"`.\n",
    "\n",
    "The testing images, on the other hand, have no labels and should thus be located all together in a folder.\n",
    "\n",
    " - `training_folders` should be a list of folder-names we get images from, i.e. a list of strings.\n",
    " - `testing_folder` should be a single folder, i.e. a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "training_folders = [\"./Data/train\"] #+ [\"./Data/Type_1\", \"./Data/Type_2\", \"./Data/Type_3\"]\n",
    "testing_folder = \"./Data/test\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a preprocessing object and test whether it reads the images correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc = DataPreprocessor(training_folders=training_folders, testing_folder=testing_folder)\n",
    "preproc.test_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a good image resizing\n",
    "\n",
    "The input images are large, typically several thousands of pixels on each axis. We want to resize them to as small as possible, while allowing a human to look at the image and still be able to tell which Type it is.\n",
    "\n",
    "Here we can play with the variable `resize_shape` to choose one that does not wash out important information in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "preproc.test_resizing(crop=True, resize_shape=(150, 150, 3), index_image=114, diagram=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters for preprocessing\n",
    "\n",
    " - `data_folder` is the folder in which we save the preprocessed numpy arrays, needed for TensorFlow training.\n",
    " - `training_subfolder` is the subfolder inside `data_folder` which contains the training numpy arrays (and their labels).\n",
    " - `testing_subfolder` is the subfolder inside `data_folder` which contains the testing numpy arrays (as well as a little auxiliary information about the arrays: which image each row corresponds to, and what order the Type 1, Type 2 and Type 3 appear in the one-hot-encoded labels).\n",
    " - `optimal_resize_shape` is the best resizing shape chosen above.\n",
    " - `crop_image` specifies whether we want to save the cropped versions of the images or whether we want the full image.\n",
    " - There are a lot of training images. We thus preprocess them in batches. `batch_size` controls the number of images in each batch. Note that if some images cannot be preprocessed, some batches will contain fewer than `batch_size` images.\n",
    " - Preprocessing can take some time. `parallelize` controls whether we want to parallelize this job over all local cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_cropped_data\"\n",
    "testing_subfolder = \"/testing_cropped_data\"\n",
    "optimal_resize_shape = (150, 150, 3)\n",
    "crop_image = True\n",
    "batch_size = 2**8\n",
    "parallelize = True\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the preprocessing\n",
    "\n",
    "*N.B. The preprocessing only gets recomputed if it is unable to detect the correct number of files in the expected folders. Therefore, if you change `optimal_resize_shape` and leave all other variables intact, the preprocessing will not happen! If you want to make sure all the numpy arrays get recomptued, you should delete the folders containing the numpy-array data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "    preproc.preprocess_save(data_folder=data_folder,\n",
    "                            training_subfolder=training_subfolder,\n",
    "                            testing_subfolder=testing_subfolder,\n",
    "                            resize_shape=optimal_resize_shape,\n",
    "                            batch_size=batch_size, crop=crop_image,\n",
    "                            parallelize=parallelize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Model\n",
    "\n",
    "Since what determines the type of cancer is closely related to the colors seen in the images (bright red if often indicative of Type 1 whereas all white is indicative of Type 3), as a benchmark model we'll perform a random forest precisely on the average-pixel RGB of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from benchmark_model import BenchmarkModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the BenchmarkModel object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "benchmdl = BenchmarkModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model. In order to do this, we need to specify:\n",
    "\n",
    " - `totnumbatches` is not strictly speaking an input for training, just a convenience variable which counts the total number of training that we have.\n",
    " - `training_batches` is a list speciying the set of batches we use for training, i.e. each element of `training_batches` is an integer.\n",
    " - `leftright` is a boolean specifying whether we also train on images that have bee flipped left-to-right.\n",
    " - `updown` is a boolean specifying whether we also train on images that have bee flipped upside-down.\n",
    " - `agnosticic_average` lets us average the predicted probabilities with the agnostic probabilities (which assign equal probability to all categories). `agnosticic_average` is an int which specifies how many times we average with the agnostic probability.\n",
    " \n",
    "The validation set can be specified in one of two ways. Either we can specify a batch number to be validation set, or we can feed it a specific array of our choice.\n",
    "\n",
    " - `validation_batch` is the integer specifying which validation batch we use for validation.\n",
    " - Alternatively, we can specify `validation_inputarray` as the input array (with shape `(number of data points, size of x-axis, size of y-axis, number of color channels)`). We then also need to specify the validation labels with `validation_labels` (which should have shape `(number of data points, number of output channels)`).\n",
    " \n",
    "***N.B. It's important you ONLY set `validation_batch`, or ONLY set `validation_inputarray` and `validation_labels`!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "totnumbatches = benchmdl.count_training_batches(\"./TensorFlow_data/training_data\")\n",
    "training_batches = range(1, totnumbatches)\n",
    "leftright = True\n",
    "updown = True\n",
    "agnosticic_average = 3\n",
    "\n",
    "validation_batch = 0\n",
    "validation_inputarray = []\n",
    "validation_labels = []\n",
    "#benchmdl.model_name = \"random_forest\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the training on a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "(benchmark_accuracy,\n",
    " benchmark_train_loss,\n",
    " benchmark_val_loss) = benchmdl.train(training_batches=training_batches,\n",
    "                                      leftright=leftright, updown=updown,\n",
    "                                      validation_inputarray=validation_inputarray,\n",
    "                                      validation_labels=validation_labels,\n",
    "                                      validation_batchnum=validation_batch,\n",
    "                                      agnosticic_average=agnosticic_average)\n",
    "print(\"Accuracy: {}\\nTraining loss: {}\\nValidation loss: {}\".format(benchmark_accuracy,\n",
    "                                                                    benchmark_train_loss,\n",
    "                                                                    benchmark_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for a completely agnostic probability would be 1.0986. Hence, we see that we do slightly better than random, but barely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on test set¶\n",
    "\n",
    "We may now use the trained model to make a prediction on the test set.\n",
    "\n",
    " - `load_test_set` is the full path to the numpy data file which contains the test-set data.\n",
    " - Alternatively, the test set can be specified with the variable `test_set`.\n",
    " - `save_prob_location` is the full path to the folder in which we want to save the predicted probabilities. These may be useful for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "load_test_set = \"./TensorFlow_data/testing_data/testing_images.npy\"\n",
    "test_set = []\n",
    "save_prob_location = \"./BenchmarkProbabilities\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a prediction on the test set and save the predicted probabilities in the folder specified by `save_prob_location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "from modules.path_munging import create_folder\n",
    "import numpy as np\n",
    "\n",
    "create_folder(save_prob_location)\n",
    "benchmdl_probabilities = benchmdl.test(load_test_set=load_test_set, test_set=test_set,\n",
    "                              agnosticic_average=agnosticic_average)\n",
    "np.save(save_prob_location + \"/benchmarkprobs.npy\", benchmdl_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We construct a variety of neural networks and study their behavior to the amount and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from convnet import ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LetNet-like networks\n",
    "\n",
    "We will first study simple LeNet-like convolutional neural networks, which consist of a couple of convolutional layers (with max pooling) followed by a few fully-connected layers.\n",
    "\n",
    "After constructing a simple network, we'll begin by checking how the validation performance of the network is affected by the quantitity of training data, to determine whether the network needs to be more complex or whether we need to increase the amount of data we train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify parameters\n",
    "\n",
    "We will train on a network with a specific architecture. To speed up training, we will train with a larger learning rate, and fine-tune our training with a smaller learning rate.\n",
    "\n",
    " - `input_shape` is the dimension-shape of the input image arrays.\n",
    " - `output_channels` is the number of output categories (i.e. cervix Types).\n",
    " - `convolutional_layers` specifies the parameters for the convolutional + max pooling layers. Each layer has the form  \n",
    " ```[int (number of output channels),  \n",
    "    tuple of length 2 (size of conv filter),  \n",
    "    tuple of length 2 (step size of conv filter),  \n",
    "    tuple of length 2 (size of max pooling filter),  \n",
    "    tuple of length 2 (step size max pooling filter)]```\n",
    " - `connected_layers` specifies the parameters for the fully connected layers. It is a list where each element is the number of neurons in the layer.\n",
    " - `keep_prob` is the dropout keep_prob (usually recommended to be 0.5).\n",
    " - `learning_rate`is the learning rate. We can change this later if needed.\n",
    " - `model_name` is the model's name, used when saving and loading trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "input_shape = (150, 150, 3)\n",
    "output_channels = 3\n",
    "convolutional_layers = [[20, (4, 4), (1, 1), (2, 2), (2, 2)],\n",
    "                        [40, (4, 4), (1, 1), (2, 2), (2, 2)],\n",
    "                        [60, (4, 4), (1, 1), (2, 2), (2, 2)]]\n",
    "connected_layers = [200, 100, 30]\n",
    "keep_prob = 0.5\n",
    "learning_rate = 0.001 # standard value is 0.001\n",
    "model_name = \"lenet_3x2\" # This name means we have 3 convolutional layers and 2 fully connected layers\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the network with the properties specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "simple_network = ConvNet(input_shape=input_shape,\n",
    "                         output_channels=output_channels, \n",
    "                         convolutional_layers=convolutional_layers,\n",
    "                         connected_layers=connected_layers, \n",
    "                         keep_prob=keep_prob,\n",
    "                         learning_rate=learning_rate,\n",
    "                         model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We will now train the neural network. We begin by specifying the parameters for training.\n",
    "\n",
    " - `data_folder` is the folder in which we save the preprocessed numpy arrays, needed for TensorFlow training.\n",
    " - `training_subfolder` is the subfolder inside `data_folder` which contains the training numpy arrays (and their labels).\n",
    " - `testing_subfolder` is the subfolder inside `data_folder` which contains the testing numpy arrays (as well as a little auxiliary information about the arrays: which image each row corresponds to, and what order the Type 1, Type 2 and Type 3 appear in the one-hot-encoded labels).\n",
    " - `size_of_minibatch` is the size of each training-step minibatch, i.e. the number of input images in each traininig step\n",
    "\n",
    "The validation set can be specified in one of two ways. Either we can specify a batch number to be validation set, or we can feed it a specific array of our choice.\n",
    "\n",
    " - `validation_batch` is the integer specifying which validation batch we use for validation.\n",
    " - Alternatively, we can specify `validation_inputarray` as the input array (with shape `(number of data points, size of x-axis, size of y-axis, number of color channels)`). We then also need to specify the validation labels with `validation_labels` (which should have shape `(number of data points, number of output channels)`).\n",
    " \n",
    "***N.B. It's important you ONLY set `validation_batch`, or ONLY set `validation_inputarray` and `validation_labels`!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from modules.neural_network import oversample\n",
    "\n",
    "# ===================== USER INPUT =====================\n",
    "data_folder = \"./TensorFlow_data\"\n",
    "training_subfolder = \"/training_data\"\n",
    "testing_subfolder = \"/testing_data\"\n",
    "size_of_minibatch = 2**6\n",
    "totnumbatches = simple_network.count_training_batches(data_folder + training_subfolder)\n",
    "validation_batch = 0\n",
    "validation_inputarray = []\n",
    "validation_labels = []\n",
    "# If we specified the validation set by hand, we might like to balance it by oversampling\n",
    "#(validation_inputarray, validation_labels) = oversample(validation_inputarray, validation_labels)\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether the network correctly loads the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "simple_network.test_loading(batch_and_index=(0, 19), batch_loc=(data_folder+training_subfolder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit the network on a single batch\n",
    "\n",
    "To choose good parameters for training, it helps to first try and overfit the network on a batch. This will also help us check if everything is working correctly.\n",
    "\n",
    "Each time we train we specify:\n",
    "\n",
    " - `epochs` is the number of epochs we train for.\n",
    " - `load_saved_model` is used for loading a saved model. It is the full path to the saved model, including epoch number, e.g. `./TensorFlow_data/trained_models/mysavedmodel-20`. If we do not want to load a model, we set `load_saved_model=\"\"`.\n",
    " - `training_batches` is a list speciying the set of batches we use for training, i.e. each element of `training_batches` is an integer.\n",
    " - `leftright` is a boolean specifying whether we also train on images that have bee flipped left-to-right.\n",
    " - `updown` is a boolean specifying whether we also train on images that have bee flipped upside-down.\n",
    " - `save_model` is a boolean specifying whether we should save our trained model as we train. The ConvNet saves the model every ten epochs, as well as the final epoch.\n",
    " - `model_destination_folder` is the folder in which we want to save our trained model. If we are loading a saved model and further training it, this variable can be set to `model_destination_folder=\"\"`, which will save the further trained models into the same folder as `load_saved_model`. \n",
    " - `printout` is a boolean which sets whether we want printouts with loss and accuracy as we train.\n",
    " - We set the learning rate, dropout rate and model name for this training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "epochs = 20\n",
    "load_saved_model = \"./TensorFlow_data/verylargemodel/lenet_3x3-50\"\n",
    "training_batches = range(1, totnumbatches)\n",
    "leftright = True\n",
    "updown = True\n",
    "save_model = True\n",
    "model_destination_folder = \"./TensorFlow_data/verylargemodel\"\n",
    "printout = True\n",
    "#simple_network.learning_rate = 0.001\n",
    "#simple_network.keep_prob = 0.5\n",
    "simple_network.model_name = \"lenet_3x3\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which performs the training according to the parameters we have just specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "# These modules will be useful later\n",
    "from modules.visualization import plot_accuracy_trainloss_valloss\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "\n",
    "def training_round(accuracies, train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Performs the neural networks training according to the specifications set by the global variables.\n",
    "    Takes three lists:\n",
    "        accuracies: list of accuracy values, for each epoch of previously-done training\n",
    "        train_losses: list of training-set losses for each epoch of previous training\n",
    "        val_losses: list of validation-set losses for each epoch of previous traning.\n",
    "    Returns:\n",
    "        Updated lists of accuracies, train_losses, val_losses, after having performed an additional set \n",
    "        of epochs of training.\n",
    "    \"\"\"\n",
    "    #Work out whether we specify the validation set using a batch number or a manually set array\n",
    "    try:\n",
    "        global validation_batch\n",
    "        validation_inputarray = []\n",
    "        validation_labels = []\n",
    "    except:\n",
    "        validation_batch = 0\n",
    "        validation_inputarray = validation_inputarray\n",
    "        validation_labels = validation_labels\n",
    "    \n",
    "    # Perform the training\n",
    "    (accuracy_list,\n",
    "    training_losses,\n",
    "    validation_losses) = simple_network.train(epochs=epochs,\n",
    "                                              load_saved_model=load_saved_model,\n",
    "                                              training_batches=training_batches,\n",
    "                                              leftright=leftright, updown=updown,\n",
    "                                              size_of_minibatch=size_of_minibatch,\n",
    "                                              validation_inputarray=validation_inputarray,\n",
    "                                              validation_labels=validation_labels,\n",
    "                                              validation_batchnum=validation_batch,\n",
    "                                              printout=printout,\n",
    "                                              save_model=save_model,\n",
    "                                              model_destination_folder=model_destination_folder)\n",
    "    \n",
    "    return accuracies + accuracy_list,  train_losses + training_losses, val_losses + validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we overfit and plot the results. We may need to iterate over the above cells until we have chosen parameters that overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "if isfile(model_destination_folder + \"/training_run_scores.npy\") is False:\n",
    "    accuracies, train_losses, val_losses = training_round([], [], [])\n",
    "    np.save(model_destination_folder + \"/training_run_scores.npy\",\n",
    "            np.array([accuracies, train_losses, val_losses]))\n",
    "else:\n",
    "    [accuracies, train_losses, val_losses] = np.load(model_destination_folder + \"/training_run_scores.npy\")\n",
    "\n",
    "plot_accuracy_trainloss_valloss(accuracies, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track validation loss with size of training data \n",
    "\n",
    "In order to decide whether we need more data or more hyper-parameters, we will go through the full training for an ever-increasing number of batches. We will then plot how the validation loss improves, and in particular whether it flattens out to a plateau.\n",
    "\n",
    "Here we should set `epochs` to the number of epochs that is just before we begin overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "simple_network.keep_prob = 0.5\n",
    "epochs = 100\n",
    "model_destination_folder = \"./TensorFlow_data/size_influence_models\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we perform the traninig on each subset of the data. First we train with learning rate 0.001, then we train for an equal number of epochs but with learning rate 0.0001 to fine-tune the model parameters. We store the accuracy, training loss and validation loss for each epoch of each training session in a variable called `allaccuracies`.\n",
    "\n",
    "If we have already done this, we simply load the previsouly-done evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "if isfile(model_destination_folder + \"/allaccuracies.npy\") is False:\n",
    "    allaccuracies = {}\n",
    "    \n",
    "    tot_num_training_batches = simple_network.count_training_batches(data_folder + training_subfolder)\n",
    "    for ii_max in range(1, tot_num_training_batches):\n",
    "        accuracies = []\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # Set the training batches\n",
    "        training_batches = range(ii_max)\n",
    "        print(training_batches)\n",
    "        # The model name is of the form lenet2x2_batch01234 if training batches [0, 1, 2, 3, 4]\n",
    "        simple_network.model_name = \"lenet2x2_batch\" + \"\".join([str(num) for num in range(ii_max)])\n",
    "        # First we don't load any model and train with learning rate 0.001\n",
    "        load_saved_model = \"\"\n",
    "        simple_network.learning_rate = 0.001\n",
    "        accuracies, train_losses, val_losses = training_round(accuracies, train_losses, val_losses)\n",
    "        #Now we load the model we just trained and train another set of epochs with learning rate 0.0001\n",
    "        load_saved_model = model_destination_folder + \"/\" + simple_network.model_name + \"-\" + str(epochs)\n",
    "        simple_network.learning_rate = 0.0001\n",
    "        accuracies, train_losses, val_losses = training_round(accuracies, train_losses, val_losses)\n",
    "        # We store the set of accuracies, traininf losses and validation losses for future comparison\n",
    "        allaccuracies.update({simple_network.model_name: [accuracies, train_losses, val_losses]})\n",
    "    np.save(model_destination_folder + \"/allaccuracies.npy\", allaccuracies)\n",
    "else:\n",
    "    allaccuracies = np.load(model_destination_folder + \"/allaccuracies.npy\").all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "from modules.neural_network import best_accuracy_trainval_loss\n",
    "from modules.visualization import plot_best_scores\n",
    "\n",
    "best_accuracy, best_train_loss, best_val_loss = best_accuracy_trainval_loss(allaccuracies)\n",
    "plot_best_scores(best_accuracy, best_train_loss, best_val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see that we would benefit from more data, which we have! Our model complexity is probably sufficient for now, and our ability to capture variance in the validation set is dictated by the lack of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training for best-performing model\n",
    "\n",
    "The final model was the best. We will see how its accuracy, trainig loss and validation loss performed through the training and whether we began to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "from modules.neural_network import sort_accuracies\n",
    "\n",
    "best_training_run_scores = sort_accuracies(allaccuracies)[-1][1]\n",
    "\n",
    "plot_accuracy_trainloss_valloss(best_training_run_scores[0],\n",
    "                                best_training_run_scores[1],\n",
    "                                best_training_run_scores[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we see that we have a decent model which did not overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "\n",
    "We will now take our best trained model and make a predictions on the test set. The prediction needs to be in a format comptatible with Kaggle's [guidelines](https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening#evaluation).\n",
    "\n",
    "The predictions are made by running the data through the neural network and applying a softmax functions to the logits.\n",
    "\n",
    " - `load_saved_model` is the full path (including batch number) to the model to be used when making predictions.\n",
    " - `load_test_set` is the full path to the numpy data file which contains the test-set data.\n",
    " - Alternatively, the test set can be specified with the variable `test_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "load_saved_model = \"./TensorFlow_data/verylargemodel/lenet_3x3-35\"\n",
    "load_test_set = data_folder + testing_subfolder + \"/testing_images.npy\"\n",
    "test_set = []\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the probabilities for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "probabilities = simple_network.test(load_saved_model=load_saved_model,\n",
    "                                    load_test_set=load_test_set, test_set=test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check predictions\n",
    "\n",
    "Plot a small subset of the test set and the generated predictions to get a feel for the quality of the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "num_images_to_check = 12\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "from modules.visualization import display_image_probabilities\n",
    "\n",
    "if len(test_set) == 0 and test_set == []:\n",
    "    test_set = np.load(load_test_set)\n",
    "\n",
    "np.random.seed(22)\n",
    "indices_to_check = np.random.choice(range(len(test_set)), size=num_images_to_check)\n",
    "fig, axes = display_image_probabilities(test_set[indices_to_check],\n",
    "                                        probabilities[indices_to_check],\n",
    "                                        data_folder + testing_subfolder)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction and make submission file\n",
    "\n",
    "We are now ready to save the probabilities and make a submission to Kaggle.\n",
    "\n",
    " - `submission_folder` is the folder in which we place our submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ===================== USER INPUT =====================\n",
    "submission_folder = \"./submissions\"\n",
    "# ======================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submissions are made into the above folder as csv files, with name `submissions` followed by the date and time of the file creating, i.e. `submissions_yyyy-mm-dd_hh-mm.csv`. For example, a submission file made on 30 April 2017 at 16:40 is saved as `submissions_2017-04-30_16-40.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ============ DOES NOT REQUIRE USER INPUT =============\n",
    "from modules.path_munging import submission\n",
    "\n",
    "saved_path = submission(probabilities, data_folder + testing_subfolder, submission_folder)\n",
    "print(\"Saved submission results to: \" + saved_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 TensorFlow",
   "language": "python",
   "name": "py27tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
